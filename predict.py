import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
import shap

from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import KMeans
from math import radians, sin, cos, sqrt, atan2
from sklearn.metrics import r2_score, mean_absolute_percentage_error

import joblib
import os

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
os.makedirs("output/plots", exist_ok=True)
os.makedirs("output/models", exist_ok=True)

# üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv("cian_moscow_detailed.csv")
df["–¶–µ–Ω–∞"] = df["–¶–µ–Ω–∞"].astype(str).str.replace(r"[^\d]", "", regex=True)
df = df[df["–¶–µ–Ω–∞"] != ""]
df["–¶–µ–Ω–∞"] = df["–¶–µ–Ω–∞"].astype(float)

# üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
df["is_luxury"] = df["–¶–µ–Ω–∞"] > 3e8
df["–¶–µ–Ω–∞_–ª–æ–≥"] = np.log1p(df["–¶–µ–Ω–∞"])

# üßπ –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
df_main = df[~df["is_luxury"]].copy()
q_high = df_main["–¶–µ–Ω–∞"].quantile(0.99)
df_main = df_main[df_main["–¶–µ–Ω–∞"] < q_high]

# üîç –†–∞–∑–±–æ—Ä —ç—Ç–∞–∂–µ–π
def parse_floor(raw):
    match = re.search(r"(\d+)\s*–∏–∑\s*(\d+)", str(raw))
    return (int(match.group(1)), int(match.group(2))) if match else (None, None)

df_main[["–≠—Ç–∞–∂_—Ç–µ–∫—É—â–∏–π", "–≠—Ç–∞–∂_–≤—Å–µ–≥–æ"]] = df_main["–≠—Ç–∞–∂"].apply(lambda x: pd.Series(parse_floor(x)))
df_lux = df[df["is_luxury"]].copy()
df_lux[["–≠—Ç–∞–∂_—Ç–µ–∫—É—â–∏–π", "–≠—Ç–∞–∂_–≤—Å–µ–≥–æ"]] = df_lux["–≠—Ç–∞–∂"].apply(lambda x: pd.Series(parse_floor(x)))

# üìç –†–∞—Å—á—ë—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –º–µ—Ç—Ä–æ
metro = pd.read_csv("metro_stations.csv")

def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dlat, dlon = radians(lat2 - lat1), radians(lon2 - lon1)
    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2
    return R * 2 * atan2(sqrt(a), sqrt(1 - a))

def nearest_metro(row):
    return metro.apply(lambda m: haversine(row.lat, row.lon, m.lat, m.lon), axis=1).min()

# üß† –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
def preprocess(df):
    df = df.copy()

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø–æ–ª—è
    df.drop(columns=["–≠—Ç–∞–∂", "–ö–æ–º–Ω–∞—Ç—ã", "–ù–∞–∑–≤–∞–Ω–∏–µ", "–°—Å—ã–ª–∫–∞", "–ß–∏—Å—Ç—ã–π –∞–¥—Ä–µ—Å"], inplace=True, errors="ignore")

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–π–æ–Ω –∏ –º–µ—Ç—Ä–æ
    def extract_district(info):
        match = re.search(r"—Ä-–Ω\s([\w\-]+)", str(info))
        return match.group(1) if match else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    def extract_metro(info):
        match = re.search(r"–º\.\s([\w\- ]+)", str(info))
        return match.group(1) if match else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    df["–†–∞–π–æ–Ω"] = df["–ò–Ω—Ñ–æ"].apply(extract_district)
    df["–ú–µ—Ç—Ä–æ"] = df["–ò–Ω—Ñ–æ"].apply(extract_metro)
    df.drop(columns=["–ò–Ω—Ñ–æ"], inplace=True)

    # ‚ùó –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Å –∫—Ä–∏—Ç–∏—á–Ω—ã–º–∏ NaN
    df.dropna(subset=["–ü–ª–æ—â–∞–¥—å", "lat", "lon"], inplace=True)

    # üßº –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    for col in ["–≠—Ç–∞–∂_—Ç–µ–∫—É—â–∏–π", "–≠—Ç–∞–∂_–≤—Å–µ–≥–æ", "–¢–∏–ø –¥–æ–º–∞", "–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏", "–ü–æ—Ç–æ–ª–∫–∏"]:
        if df[col].dtype == "O":
            df[col] = df[col].fillna("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
        else:
            df[col] = df[col].fillna(df[col].median())

    # ‚ûï –ü—Ä–∏–∑–Ω–∞–∫–∏
    df["–í–æ–∑—Ä–∞—Å—Ç_–¥–æ–º–∞"] = 2025 - pd.to_numeric(df["–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏"], errors="coerce")
    df["–í–æ–∑—Ä–∞—Å—Ç_–¥–æ–º–∞"] = df["–í–æ–∑—Ä–∞—Å—Ç_–¥–æ–º–∞"].fillna(df["–í–æ–∑—Ä–∞—Å—Ç_–¥–æ–º–∞"].median())

    df["–≠—Ç–∞–∂_–¥–æ–ª—è"] = pd.to_numeric(df["–≠—Ç–∞–∂_—Ç–µ–∫—É—â–∏–π"], errors="coerce") / pd.to_numeric(df["–≠—Ç–∞–∂_–≤—Å–µ–≥–æ"], errors="coerce")
    df["–≠—Ç–∞–∂_–¥–æ–ª—è"] = df["–≠—Ç–∞–∂_–¥–æ–ª—è"].fillna(0.5)

    df["–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ_–¥–æ_—Ü–µ–Ω—Ç—Ä–∞"] = np.sqrt((df["lat"] - 55.7558)**2 + (df["lon"] - 37.6176)**2)
    df["dist_to_metro_km"] = df.apply(nearest_metro, axis=1)

    df["–ü—Ä–µ–º–∏—É–º_—Ä–∞–π–æ–Ω"] = df["–†–∞–π–æ–Ω"].isin(["–•–∞–º–æ–≤–Ω–∏–∫–∏", "–ü—Ä–µ—Å–Ω–µ–Ω—Å–∫–∏–π", "–Ø–∫–∏–º–∞–Ω–∫–∞", "–ó–∞–º–æ—Å–∫–≤–æ—Ä–µ—á—å–µ"]).astype(int)

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    kmeans = KMeans(n_clusters=15, random_state=42)
    df["–ö–ª–∞—Å—Ç–µ—Ä"] = kmeans.fit_predict(df[["lat", "lon"]])
    df["–ö–ª–∞—Å—Ç–µ—Ä"] = df["–ö–ª–∞—Å—Ç–µ—Ä"].astype(str)

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ
    categorical = ["–¢–∏–ø –¥–æ–º–∞", "–†–∞–π–æ–Ω", "–ú–µ—Ç—Ä–æ", "–ö–ª–∞—Å—Ç–µ—Ä"]
    encoder = OneHotEncoder(handle_unknown="ignore")
    encoded = encoder.fit_transform(df[categorical]).toarray()
    encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical), index=df.index)

    numeric = ["–ü–ª–æ—â–∞–¥—å", "–≠—Ç–∞–∂_—Ç–µ–∫—É—â–∏–π", "–≠—Ç–∞–∂_–≤—Å–µ–≥–æ", "–ü–æ—Ç–æ–ª–∫–∏", "–í–æ–∑—Ä–∞—Å—Ç_–¥–æ–º–∞", "–≠—Ç–∞–∂_–¥–æ–ª—è", "–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ_–¥–æ_—Ü–µ–Ω—Ç—Ä–∞", "dist_to_metro_km"]
    numeric = [col for col in numeric if col in df.columns]

    for col in numeric:
        df[col] = pd.to_numeric(df[col], errors="coerce").fillna(df[col].median())

    scaler = StandardScaler()
    scaled = scaler.fit_transform(df[numeric])
    scaled_df = pd.DataFrame(scaled, columns=numeric, index=df.index)

    features = pd.concat([scaled_df, encoded_df, df[["–ü—Ä–µ–º–∏—É–º_—Ä–∞–π–æ–Ω"]]], axis=1)
    target = np.log1p(df["–¶–µ–Ω–∞"])

    return features, target

# üöÄ –ú–æ–¥–µ–ª—å —Å—Ç—ç–∫–∏–Ω–≥
X_main, y_main = preprocess(df_main)
X_train, X_test, y_train, y_test = train_test_split(X_main, y_main, test_size=0.2, random_state=42)

xgb = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=6, random_state=42)
lgbm = LGBMRegressor(n_estimators=300, learning_rate=0.1, max_depth=6, random_state=42)
rf = RandomForestRegressor(n_estimators=100, random_state=42)

model_main = StackingRegressor(
    estimators=[("xgb", xgb), ("lgbm", lgbm), ("rf", rf)],
    final_estimator=XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=4, random_state=42),
    passthrough=True,
    n_jobs=-1
)
model_main.fit(X_train, y_train)

# üíé Luxury –º–æ–¥–µ–ª—å
X_lux, y_lux = preprocess(df_lux)
model_lux = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=6, random_state=42)
model_lux.fit(X_lux, y_lux)

# üìà –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
y_pred_main = np.expm1(model_main.predict(X_test))
y_test_true = np.expm1(y_test)
mae_main = mean_absolute_error(y_test_true, y_pred_main)

y_pred_lux = np.expm1(model_lux.predict(X_lux))
y_lux_true = np.expm1(y_lux)
mae_lux = mean_absolute_error(y_lux_true, y_pred_lux)

print(f"\nüìä MAE –Ω–∞ –æ–±—ã—á–Ω—ã—Ö –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö: {mae_main:,.0f} ‚ÇΩ")
print(f"üíé MAE –Ω–∞ luxury-—Å–µ–≥–º–µ–Ω—Ç–µ: {mae_lux:,.0f} ‚ÇΩ")

# üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
comparison_main = pd.DataFrame({"–†–µ–∞–ª—å–Ω–∞—è (‚ÇΩ)": y_test_true, "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (‚ÇΩ)": y_pred_main})
comparison_lux = pd.DataFrame({"–†–µ–∞–ª—å–Ω–∞—è (‚ÇΩ)": y_lux_true, "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (‚ÇΩ)": y_pred_lux})
print("\nüîç –û–±—ã—á–Ω—ã–µ –∫–≤–∞—Ä—Ç–∏—Ä—ã:")
print(comparison_main.head(5))
print("\nüîç Luxury –∫–≤–∞—Ä—Ç–∏—Ä—ã:")
print(comparison_lux.head(5))

print(f"–û–±—ä–µ–∫—Ç–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞: {len(X_main)} (–≤—Å–µ–≥–æ)")
print(f"–û–±—ä–µ–∫—Ç–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {len(X_test)}")
print(f"R¬≤: {r2_score(y_test_true, y_pred_main):.3f}")
print(f"MAPE: {mean_absolute_percentage_error(y_test_true, y_pred_main):.2%}")
print(f"R¬≤(LUX): {r2_score(y_lux_true, y_pred_lux):.3f}")
print(f"MAPE(LUX): {mean_absolute_percentage_error(y_lux_true, y_pred_lux):.2%}")


# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
importances = model_main.named_estimators_["xgb"].feature_importances_
feature_names = X_train.columns
importance_df = pd.DataFrame({"–ü—Ä–∏–∑–Ω–∞–∫": feature_names, "–í–∞–∂–Ω–æ—Å—Ç—å": importances}).sort_values("–í–∞–∂–Ω–æ—Å—Ç—å", ascending=False).head(20)
plt.figure(figsize=(10, 6))
sns.barplot(x="–í–∞–∂–Ω–æ—Å—Ç—å", y="–ü—Ä–∏–∑–Ω–∞–∫", data=importance_df)
plt.title("–¢–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (XGBoost)")
plt.tight_layout()
plt.savefig("output/plots/feature_importance.png")


# –û—à–∏–±–∫–∏
errors = np.abs(y_test_true - y_pred_main)
plt.figure(figsize=(8, 5))
sns.histplot(errors, bins=30, kde=True, color="crimson")
plt.title("–ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏")
plt.tight_layout()
plt.savefig("output/plots/missing.png")


# –ü–ª–æ—Ç–Ω–æ—Å—Ç–∏
plt.figure(figsize=(8, 5))
sns.kdeplot(y_test_true, label="–†–µ–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞")
sns.kdeplot(y_pred_main, label="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è")
plt.legend()
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (–û–±—ã—á–Ω—ã–µ)")
plt.tight_layout()
plt.savefig("output/plots/plot(stangart).png")


plt.figure(figsize=(8, 5))
sns.kdeplot(y_lux_true, label="–†–µ–∞–ª—å–Ω–∞—è (Luxury)")
sns.kdeplot(y_pred_lux, label="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è (Luxury)")
plt.legend()
plt.title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (Luxury)")
plt.tight_layout()
plt.savefig("output/plots/plot_lux.png")


# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
joblib.dump(model_main, "output/models/model_main.joblib")
joblib.dump(preprocess, "output/models/preprocess_func.joblib")
joblib.dump(X_test, "output/models/X_test.joblib")
joblib.dump(y_test_true, "output/models/y_test_true.joblib")
joblib.dump(y_pred_main, "output/models/y_pred_main.joblib")
joblib.dump(df_main, "output/models/df_main.joblib")
